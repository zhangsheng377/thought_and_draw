# Thought and Draw
---------------------
## 前言
在前一段时间玩Stadle Diffusion时，我发现这种sd架构的多模态模型其实对于语言的理解能力并不好（传统的sd xl模型甚至要把语言描述浓缩为一个一个独立的词，flux模型稍好一些，但依然不能通过语言精确地控制图像的生成）。

![sd架构](https://i-blog.csdnimg.cn/blog_migrate/3c06a151a42ee33e1e7f94bcd0ef051a.png#pic_center)
![Stable Diffusion中的CrossAttention结构](https://picx.zhimg.com/v2-4073d371ab72a7b1cc855a1c149b1d73_r.jpg){:height="400px" width="200px"}
我觉得，其根本原因应该是因为sd架构利用文本编码器（一般是clip，产生77*768维度）（77个词太小了，就算分组叠加取平均，还是破坏了深层语义）所生成的向量，只是简单的用cross-attention将文本向量与图片向量相融合，这样势必会导致以下三种结果：

1. 模型将这77个文本embedding相互独立看待，所以生成的要去除的噪声图像就只是趋近于这77个词的语义空间，那么就会导致文本prompt变成一个一个词才是最适配模型的。（现状）
![在左下角生成一朵花](./readme_pic/在左下角生成一朵花.png)
2. 模型需要近乎咒语般的prompt和随机数种子，才能随机出一个想要的样子，但这本质上只是撞大运，而不是精确的上下文控制。（现状）
3. 模型在这有限长度的文本上下文中，只靠一个cross-attention就幸运的成功学到了精确的细节控制（词语此之间的相互语义，例如“在左下角生成一朵花”）。（我觉得不太可能，应该要附加更深的结构（本文间的自注意力）、更大的上下文）（话说gpt-4o貌似不错，不知道是不是改成我想的纯transformer结构了）

总而言之，现在的sd架构缺少真正的上下文控制方法，虽然不排除继续在sd架构上魔改、打补丁也能修复这一问题，但我想提出一个新的思路：

<font size=5>**直接用纯transformer架构，混合生成文本和图像**</font>





---------------------

## TODO

- [ ] 分块/分层，thought并生成该块图像，理论上可以有更好效果及更大分辨率
